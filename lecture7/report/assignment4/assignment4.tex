\documentclass[class=jsarticle, crop=false, dvipdfmx, fleqn]{standalone}
\input{/Users/User/Documents/University/report_template/preamble/preamble}
\begin{document}

\section*{宿題4}


以下の深層学習に関する技術用語について説明する。
これらを講義で説明した多層ニューラルネットワークに組み込む方法についても検討する。


\subsection*{Dropout}

Dropoutは，訓練時にニューロンのうち一部を確率的に不活性化させて学習させることにより，
汎化性能を上げる手法である。
つまり，Dropoutを適用する層において，
重みのうち一部を確率的に選び，
選ばれたものの重みを0として順伝搬を行い，
また逆伝搬時にも重みの更新をしないようにする。
ここで，Dropoutする確率はハイパーパラメータとなる。
推論時には，
全てのノードを用いて計算を行った後，
訓練時にDropoutしていた割合を出力にかけることで，
用いているノードの数が訓練時より増加し大きくなった出力の値を修正している。
このように学習することによって，
複数のネットワークを独立に学習し，
推論時にその出力を平均する，
アンサンブル学習の近似になるといわれている。

これを多層ニューラルネットワークに組み込もうとするときは，
単純に層の後にこれを入れればよい。
なお，どのノードを不活性化したかを記憶しておく必要があることに注意である。



\subsection*{momentum}

パラメータの更新はSGD以外の方法によっても行うことができる。
momentumはその手法の1つで，直訳すると運動量である。
momentumによるパラメータの更新式は以下の式で表される（$\alpha$はハイパーパラメータ）。
\begin{align}
    & \bm{v} \leftarrow \alpha \bm{v} - \eta \pdv{E}{\bm{\theta}} \\
    & \bm{\theta} \leftarrow \bm{\theta} + \bm{v}
\end{align}
式からわかるように，
momentumでは更新に用いる値を計算された勾配の値そのものからその指数平均へと変更している。
これにより，振動が抑制され，損失関数が極小値に向かいやすくなるという手法である。
パラメータが張る空間上での損失関数の値の動きを考えると，
これは慣性が働いているかのような動きとなる。

これを多層ニューラルネットワークに組み込もうとするときは，
パラメータの更新式をSGDからこれに取り換えればよい。
\(\bm{v}\)の値を記憶しておかなければならないことに注意である。



\subsection*{data augmentation}

data augmentationとは，データを増やす手法のことである。
これは，新規にデータを集めるということではなく，
現在手元にあるデータに対し何かしらの処理を加えることでデータを増やすということである。
この手法は当然データの種類によって様々であるが，
例えば画像について考えると，
次のようなものが挙げられる

\begin{itemize}
    \item 上下・左右の反転
    \item 回転
    \item ノイズの付加
    \item 色の反転
    \item Random Crop（画像の一部を切り抜く）
    \item CutOut（画像の一部にマスクをかける）
\end{itemize}

当然，これらの手法は状況に応じて使い分けられるべきである。
例えば，手書き文字認識をしたいときには，左右の反転は用いられないだろう。

これは前処理にあたる部分なので，
多層ニューラルネットワークに組み込もうとするときは，
その入力に対する操作となる。
例えばMNISTに対しては，ノイズの付加や多少の回転は適用してみてもよい手法であると考えられる。



\end{document}
